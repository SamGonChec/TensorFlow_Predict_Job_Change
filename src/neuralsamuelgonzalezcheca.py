# -*- coding: utf-8 -*-
"""NeuralSamuelGonzalezCheca.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1SyOA85rqTbPiVrBN7ur_PGv7T-qNs-P0
"""

from google.colab import drive
drive.mount('/content/drive')

import tensorflow as tf
import pandas as pd
import tensorflow_datasets as tfds
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras import Model
import tensorflow_datasets as tfds
from sklearn import preprocessing
import numpy as np
from tensorflow.keras import regularizers


#this data was encoded the url for the encoding done is: https://colab.research.google.com/drive/1a8sMMT9-6ERbQwwnPTfoRdHTAdtKLGwd?usp=sharing
reader = pd.read_csv('/content/drive/MyDrive/CS461/encoded_data.csv')

#these two lines of code I used for debugging I used the answer to find why my accuracy was stuck at 75% and loss was at nan: https://datascience.stackexchange.com/questions/68331/keras-sequential-model-returns-loss-nan 
#reader.isnull().any()

#https://stackoverflow.com/questions/52747672/normalize-pandas-dataframe-at-specific-columns
#https://stackoverflow.com/questions/26414913/normalize-columns-of-pandas-data-frame
x = reader[['experience','last_new_job','training_hours']]
min_max_scaler = preprocessing.MinMaxScaler()
x_scaled = min_max_scaler.fit_transform(x)
dataset = pd.DataFrame(x_scaled)
reader['experience'] = dataset[0]
reader['last_new_job'] = dataset[1]
reader['training_hours'] = dataset[2]

#i tried using this Tensorflow method but it kept crashing
#reader = tf.one_hot(reader, depth=reader.size)

#randomized data and divided it into sections source: https://datatofish.com/random-rows-pandas-dataframe/
def shuffData(reader):
  temp = reader
  train_data = temp.sample(frac=.7)
  temp = temp.drop(train_data.index)
  test_data = temp.sample(frac=.5)
  validation = temp.drop(test_data.index)
  return train_data, test_data, validation

#grabbed the specific target columns from each divided section of the data, source: https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.pop.html
def setTarget(train_data,test_data,validation):
  check = train_data.pop('target')
  dataX = train_data

  test_check = test_data.pop('target')
  data_test = test_data

  validate_check = validation.pop('target')
  validate_data = validation
  return check, test_check, validate_check

#I was having issues converting the data so I decied to use this from the tensorflow site:https://www.tensorflow.org/api_docs/python/tf/keras/utils/to_categorical
def encodeData(check,test_check,validate_check):
  encode_train = tf.keras.utils.to_categorical(check)
  encode_test = tf.keras.utils.to_categorical(test_check)
  encode_validate = tf.keras.utils.to_categorical(validate_check)
  return encode_train,encode_test,encode_validate

#data assignment
train, test, valid = shuffData(reader)

targetTrain, targetTest, targetVal = setTarget(train,test,valid)

encodeTrain, encodeTest, encodeValid = encodeData(targetTrain,targetTest,targetVal)


#source of activation functions and general keras information https://keras.io/api/layers/activations/
model = tf.keras.Sequential([
        tf.keras.layers.Dense(100,activation="tanh"),
        tf.keras.layers.Dense(50,activation="tanh"),
        tf.keras.layers.Dense(2,activation="softmax")
])
model.compile(optimizer='adam',loss='mean_squared_error', metrics=['accuracy'])

#to give myself an idea on how bad it predicts before and after the training
testing_loss = model.evaluate(test,encodeTest)
print("Test loss:", testing_loss[0])
print("Test accuracy:", testing_loss[1])

valid_loss = model.evaluate(valid,encodeValid)
print("Valid loss:", valid_loss[0])
print("Valid accuracy:", valid_loss[1])

prediction = model.predict(test[:5])
print("Prediction: ", prediction[1])

#training
model.fit(train,encodeTrain,epochs=50,batch_size=10,validation_data=(valid,encodeValid))

testing_loss = model.evaluate(test,encodeTest)
print("Test loss:", testing_loss[0])
print("Test accuracy:", testing_loss[1])

valid_loss = model.evaluate(valid,encodeValid)
print("Valid loss:", valid_loss[0])
print("Valid accuracy:", valid_loss[1])

prediction = model.predict(test[:5])
print("Prediction: ", prediction[1])

reader.to_csv('encoded_data.csv');